# -*- coding: utf-8 -*-
"""Projek - Copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/jalikarani/projek/blob/main/Projek_Copy.ipynb

# Import Library yang dibutuhkan

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

data_wine=pd.read_csv('winequality-red.csv')

data_wine

"""#DATA UNDERSTANDING"""

list(data_wine.columns)

data_wine.shape

data_wine.dtypes

data_wine.info()

data_wine.isnull().sum()

data_wine.head()

data_wine.tail()

data_wine.describe()

data_wine.quality.value_counts().sort_values(ascending=True)

sns.set()
sns.catplot(x='quality', data=data_wine, kind='count', height=9)

#bisa juga -> sns.countplot(x='quality', data=data_wine)

data_wine.quality.unique()

plt.figure(figsize=(10,8))
sns.barplot(x='quality', y='alcohol', data=data_wine)

plt.figure(figsize=(10,8))
sns.barplot(x='quality', y='fixed acidity', data=data_wine)

plt.figure(figsize=(10,8))
sns.barplot(x='quality', y='volatile acidity', data=data_wine)

plt.figure(figsize=(10,8))
sns.barplot(x='quality', y='citric acid', data=data_wine)

plt.figure(figsize=(10,8))
sns.barplot(x='quality', y='residual sugar', data=data_wine)

plt.figure(figsize=(10,8))
sns.barplot(x='quality', y='chlorides', data=data_wine)

plt.figure(figsize=(10,8))
sns.barplot(x='quality', y='pH', data=data_wine)

plt.figure(figsize=(10,8))
sns.barplot(x='quality', y='free sulfur dioxide', data=data_wine)

data_wine.head()

plt.figure(figsize=(12,9))
sns.boxplot(x='quality', data=data_wine)

data_wine.hist(bins=50, figsize=(15,17), color='r')
plt.show()

data_wine.corr()

data_wine.corr().quality.sort_values()

plt.figure(figsize=(15, 10))
sns.heatmap(data_wine.corr(), annot=True, cmap='Spectral')

sns.pairplot(data_wine)



"""# Pre-Processing"""

# separate the data and Label
X = data_wine.drop('quality',axis=1)

X

Y= data_wine['quality'].apply(lambda y: 1 if y>=7 else 0)
Y

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)

print(data_wine.shape)
print(X.shape)
print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)

"""#DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

dt = DecisionTreeClassifier(max_depth=4, random_state=10)
dt.fit(X_train, Y_train)
dt_pred = dt.predict(X_test)
dt_acc = accuracy_score(dt_pred, Y_test)
print('Accuracy :{:.2f}%'.format(dt_acc*100))

from sklearn import tree

tree.plot_tree(dt)

"""#RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, Y_train)
rf_pred = rf.predict(X_test)
rf_acc = accuracy_score(rf_pred, Y_test)
print('Accuracy : {:.2f}%'.format(rf_acc*100))

"""# K-Nearest Neighbor (KNN)




"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=4)
knn.fit(X_train, Y_train)
Y_pred = knn.predict(X_test)
knn_acc = accuracy_score(Y_test,Y_pred)
print('Accuracy: {:.2f}%'.format(knn_acc*100))

"""#Logistic Regression"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(X_train, Y_train)
lr_pred = lr.predict(X_test)
lr_acc = accuracy_score(lr_pred, Y_test)
print("Accuracy: {:.2f}%".format(lr_acc*100))

"""#Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

nb = GaussianNB()
nb.fit(X_train, Y_train)
nb_pred = nb.predict(X_test)
nb_acc = accuracy_score(nb_pred, Y_test)
print("Accuracy: {:.2f}%".format(nb_acc*100))



"""#Support Vector Machines (SVM)"""

from sklearn import svm

sv = svm.SVC()
sv.fit(X_train,Y_train)
sv_pred = sv.predict(X_test)
sv_acc = accuracy_score(sv_pred, Y_test)
print("Accuracy: {:.2f}%".format(sv_acc*100))



"""#Perbandingan"""

print('Perbandingan Akurasi Setiap Model')
print('==============================================')
print('Decision Tree           : {:.2f}%'.format(dt_acc * 100))
print('Random Forest           : {:.2f}%'.format(rf_acc * 100))
print('KNN                     : {:.2f}%'.format(knn_acc * 100))
print('Logistic Regression     : {:.2f}%'.format(lr_acc * 100))
print('Naive Bayes             : {:.2f}%'.format(nb_acc * 100))
print('Support Vector Machines : {:.2f}%'.format(sv_acc * 100))

"""#Prediction"""

contoh_data= (3.2,	0.4,	0.08,	5.0,	0.090,	12.0,	90,	1,	8,	0.58,	10)
data_nparray = np.asarray(contoh_data)
reshape = data_nparray.reshape(1,-1)
predict = rf.predict(reshape)
print(predict)

if (predict[0]==1):
  print('Good quality')
else:
  print('Bad quality')

